# # Import libraries
import sys
sys.path.append('/usr/local/spark')
sys.path.append('/usr/local/spark/python')
sys.path.append('/usr/local/spark/python/lib')
sys.path.append('/usr/local/spark/python/lib/py4j-0.10.4-src.zip')

import os
os.environ['SPARK_HOME']='/usr/local/spark'


# # Import Spark libraries
import pyspark


# set configuration
import pyspark.conf

scc = pyspark.conf.SparkConf()
scc.setMaster("local")
scc.setAppName("IP top K")
scc.set("spark.executor.memory","1g")


# # Create Spark cluster context element
sc = pyspark.SparkContext(conf=scc)
sc.defaultParallelism

# # Load Text File from folder
import pandas as pd
ip_table = pd.read_table('/home/admin/Desktop/BigData/PfStateDump09-03-2017', sep=' ')
ip_table.head()
ip_table = ip_table[['all','carp','193.205.61.34','(10.11.8.250)','->','224.0.0.18']]
ip_table_list = ip_table.values.tolist()
ip_table_list[0:3]

# # Convert to RDD
ip_table_rdd = sc.parallelize(ip_table_list)
ip_table_rdd.take(1)

# # Process RDD
ip_table_rdd.filter(lambda x: x[4]=='->')

# separate `IP` and `PORT` elements
ip_table_rdd.filter(lambda x: x[4]=='->').map(lambda x:(x[0],x[1],
              x[2].split(':')[0],x[2].split(':')[1],
              x[3][1:-1].split(':')[0],x[3][1:-1].split(':')[1],
              x[5].split(':')[0],x[5].split(':')[1])).take(2)

# create a function for split records
def split_record(x):
    trans = 'none'
    external_addr = 'none'
    external_port = 'none'
    sender_addr = 'none'
    sender_port = 'none'
    destination_addr = 'none'
    destination_port = 'none'
    try:
        trans = x[1]
        external_addr = x[2].split(':')[0]
        external_port = x[2].split(':')[1]
        sender_addr = x[3][1:-1].split(':')[0]
        sender_port = x[3][1:-1].split(':')[1]
        destination_addr = x[5].split(':')[0]
        destination_port = x[5].split(':')[1]
    except IndexError:
        pass
    return (trans,external_addr,external_port,sender_addr,sender_port,destination_addr,destination_port)


# MAP each destination IP (key) with a value (1)
ip_table_rdd.filter(lambda x: x[4]=='->').map(split_record).map(lambda x:(x[5],1)).take(7)

# Reduce for count number of client for single destination IP
def sum_reducer(x,y):
    x_=x[-1]
    y_=y[-1]
    return ([x[0],y[0]],x_+y_)

destination_ip_count = ip_table_rdd.filter(lambda x: x[4]=='->').map(split_record).map(lambda x:(x[5],[x,1])).reduceByKey(sum_reducer).map(lambda x:(x[0],x[-1][-1]))

# Sort the rdd for the ip with more client
top_k= destination_ip_count.sortBy(lambda x: -x[1]).take(10)

# create a DataFrame
top_k_df=pd.DataFrame(top_k)
get_ipython().magic(u'matplotlib inline')
top_k_df.plot(x='destination_IP',y='client',kind='bar')

# # For the source IP
source_ip = ip_table_rdd.filter(lambda x:x[4]=='->').map(split_record).map(lambda x:(x[3],[x,1])).reduceByKey(sum_reducer).map(lambda x:(x[0],x[-1][-1]))
source_ip.take(3)

# Sort the rdd for the ip with more connection
top_h= source_ip.sortBy(lambda x: -x[1]).take(10)

# create DataFrame
top_h_df=pd.DataFrame(top_h)
top_h_df.columns=['source_IP','connection']
top_h_df.head()

top_h_df.plot(x='source_IP',y='connection',kind='bar')


